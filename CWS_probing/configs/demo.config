### use # to comment out the configure item

### I/O ###
train_dir=../demo/demo.seg
dev_dir=../demo/demo.seg
test_dir=../demo/demo.seg
model_dir=../demo/ncrf.seg
word_emb_dir=


#raw_dir=
#decode_dir=
#dset_dir=
#load_model_dir=
#char_emb_dir=

norm_word_emb=False
norm_char_emb=False
number_normalized=False
seg=True
word_emb_dim=50
char_emb_dim=30

###NetworkConfiguration###
use_crf=False
use_char=False
char_seq_feature=CNN
use_word_seq=False
use_word_emb=False
word_seq_feature=LSTM
low_level_transformer=none
low_level_transformer_finetune=False
#change this line to use other fine-tune models
high_level_transformer=bert-base-chinese 
high_level_transformer_finetune=False
#feature=[POS] emb_size=20
#feature=[Cap] emb_size=20
#nbest=1

###TrainingSetting###
status=train
optimizer=Adam
iteration=3
batch_size=8
ave_batch_loss=False

###Hyperparameters###
cnn_layer=4
char_hidden_dim=50
hidden_dim=768
dropout=0.1
lstm_layer=2
bilstm=True
learning_rate=2e-5
lr_decay=0.05
momentum=0
l2=0
#gpu
#clip=

